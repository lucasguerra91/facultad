Content-Type: text/x-zim-wiki
Wiki-Format: zim 0.4

====== Capitulo 2 ======
Created martes 27 marzo 2018

=== PROCESOS Y THREADS ===

== Procesos ==
Todos los ordenadores modernos pueden hacer varias cosas a la vez. Mientras un ordenador está ejecutando un programa de usuario puede perfectamente también estar leyendo de un disco e imprimiendo texto en una pantalla o una impresora. En un sistema multiprogramado la CPU también conmuta de unos programas a otros, ejecutando cada uno de ellos durante decenas o cientos de milisegundos. Aunque, estrictamente hablando, en cualquier instante de tiempo la CPU sólo está ejecutando un programa, en el transcurso de 1 segundo ha podido estar trabajando sobre varios programas, dando entonces a los usuarios la impresión de un cierto paralelismo. En este contexto a veces la gente habla de pseudoparalelismo, en contraste con el auténtico paralelismo del hardware de los sistemas multiprocesador (que tienen dos o más CPUs compartiendo la misma memoria física). Seguir la pista de múltiples actividades paralelas resulta muy complicado para las personas. Por ese motivo los diseñadores del sistema operativo han desarrollado a través de los años un modelo conceptual evolucionado (el de los procesos secuenciales) que permite tratar el paralelismo de una forma más fácil. Este modelo, sus usos, y algunas de sus consecuencias constituyen el tema de este capítulo.

== El modelo de los Procesos Secuenciales ==
En este modelo, todo el software ejecutable en el ordenador, incluyendo a veces al propio sistema operativo, se organiza en un número de procesos secuenciales, o simplemente procesos para acortar. Un proceso es justamente un programa en ejecución, incluyendo los valores actuales del contador de programa, registros y variables. Conceptualmente cada proceso tiene su propia CPU virtual. En realidad, por supuesto, la CPU real conmuta sucesivamente de un proceso a otro, pero para entender el sistema, es mucho más fácil pensar sobre una colección de procesos ejecutándose en (pseudo) paralelo, que intentar seguir la pista de cómo la CPU conmuta de un programa a otro. Esta rápida conmutación de un proceso a otro en algún orden se denomina **multiprogramación.**

Con la CPU conmutando de un proceso a otro, la velocidad a la cual un proceso realiza su computación no es uniforme y probablemente ni siquiera es reproducible si los mismos procesos se ejecutan de nuevo. Por ese motivo los procesos no deben programarse bajo suposiciones preconcebidas sobre su velocidad de ejecución. 
Consideremos, por ejemplo, un proceso de E/S que restaura los ficheros de un backup en cinta. El proceso comienza poniendo
en marcha la cinta, ejecuta 10.000 veces un bucle para esperar a que la cinta adquiera la velocidad adecuada, y en ese preciso momento envía un comando para leer el primer registro de la cinta. Si la CPU decide conmutar a otro proceso durante el bucle de retardo, el proceso de la cinta puede no volver a ejecutarse antes de que el primer registro sobrepase la cabeza de lectura.
Cuando un proceso tiene requerimientos de tiempo real críticos como el anterior, esto es, que ciertos sucesos particulares deben ocurrir dentro de un número de milisegundos especificado, entonces es necesario tomar medidas especiales para asegurar que efectivamente esos sucesos ocurran dentro de esos límites de tiempo. Sin embargo, normalmente la mayoría de los procesos
no se ven afectados por la multiprogramación subyacente de la CPU o por las velocidades relativas de los diferentes procesos.

**La diferencia entre un proceso y un programa** es sutil, pero crucial. Para explicar esto puede servirnos de ayuda una analogía. Consideremos un científico informático con aptitudes culinarias que está preparando una tarta de cumpleaños para su hija. Para ello dispone de una receta de la tarta de cumpleaños y una cocina bien surtida con todos los ingredientes: harina, huevos, azúcar, extracto de vainilla, etc. En esta analogía, la receta representa el programa (es decir un algoritmo expresado mediante alguna notación apropiada), el científico informático representa el procesador (CPU), y los ingredientes de la tarta representan los datos de entrada.
El proceso es la actividad consistente en nuestro pastelero leyendo la receta, añadiendo los
ingredientes y preparando la tarta. Imaginemos ahora que el hijo del científico informático entra corriendo y gritando, diciendo que le ha picado una abeja. El científico informático apunta por donde iba en la receta (salva el estado del proceso actual), coge un libro de primeros auxilios y comienza a seguir las instrucciones para la cura. Aquí vemos cómo el procesador conmuta de un proceso (preparar la tarta) a un proceso de mayor prioridad (administrar cuidados médicos), cada uno de los cuales sigue un programa diferente (la receta frente al libro de primeros auxilios). 
Una vez que termina de curar la picadura de la abeja, el científico vuelve a su tarta, continuando en el punto donde la dejó.

**La idea clave aquí es que un proceso es una actividad de algún tipo. Tiene un programa,entrada, salida y un estado. Un único procesador puede compartirse entre varios procesosutilizando un algoritmo de planificación que determine cuándo hay que detener el trabajo sobre un proceso y pasar a atender a otro diferente.**


== Creacion de procesos ==
Los sistemas operativos necesitan asegurar de alguna forma que puedan existir todos los procesos necesarios. En sistemas muy sencillos, o en sistemas diseñados para ejecutar tan solo una única aplicación (por ejemplo el controlador de un microondas), puede conseguirse que cuando el sistema termine de arrancar estén presentes ya todos los procesos que puedan necesitarse en el futuro. Sin embargo, en sistemas de propósito general es necesaria alguna manera de poder crear y destruir los procesos según sea necesario durante la operación del sistema. 
Vamos a fijarnos ahora en algunas de estas cuestiones.

**Los cuatro principales sucesos que provocan la creación de nuevos procesos son:**
1. La inicialización del sistema
2. La ejecución por parte de un proceso (en ejecución) de una llamada al sistema de
creación de un nuevo proceso.
3. La petición por parte del usuario de la creación de un nuevo proceso.
4. El inicio de un trabajo en batch.

Cuando un sistema operativo arranca, se crean típicamente varios procesos. Algunos de esos procesos son procesos de superficie (o en primer plano), esto es, procesos que interactúan con los usuarios (humanos) y realizan trabajo para ellos. Otros son procesos de fondo (o en segundo plano), que no están asociados con usuarios particulares, sino que tienen alguna función específica. Por ejemplo, un proceso de fondo puede diseñarse para que se encargue de aceptar el correo electrónico entrante, de manera que esté durmiendo la mayor parte del día pero vuelva repentinamente a la vida tan pronto como llegue algún correo. Otro proceso de fondo puede diseñarse para aceptar peticiones entrantes de páginas web residentes en esa máquina, despertándose cada vez que llegue una nueva petición para servir una cierta página. Los procesos que se ejecutan como procesos de fondo para llevar a cabo alguna actividad tal como el correo electrónico, las páginas web, las news o la impresión de ficheros de salida, etc, se denominan demonios. Los sistemas grandes tienen comúnmente docenas de ellos. En UNIX, el programa ps puede utilizarse para listar los procesos que están en marcha. En Windows 95/98/Me tecleando CTRL-ALT-SUPR una vez, se muestra todo lo que está en marcha. En Windows 2000 se utiliza el administrador de tareas.


Adicionalmente a los procesos creados en el momento del arranque, también pueden crearse nuevos procesos después. A menudo un proceso en ejecución puede hacer llamadas al sistema para crear uno o más procesos nuevos para que le ayuden en su trabajo. Crear nuevos procesos es particularmente útil cuando el trabajo a realizar puede formularse fácilmente en términos de varios procesos relacionados, pero por otra parte independientes, que interactúan entre sí. Por ejemplo, si se está extrayendo una gran cantidad de datos a través de una red de comunicación para su procesamiento subsiguiente, puede ser conveniente crear un proceso para extraer los datos y ponerlos en un buffer compartido mientras que un segundo proceso va retirando los datos del buffer y los procesa. En el caso de un sistema multiprocesador podemos conseguir que todo el trabajo se haga efectivamente más rápido si permitimos que esos dos procesos se ejecuten cada uno en una CPU diferente.




**En UNIX** sólo existe una llamada al sistema para crear un nuevo proceso: fork. Esta llamada crea un clon (una copia exacta) del proceso que hizo la llamada. Después del fork, los dos procesos, el padre y el hijo, tienen la misma imagen de memoria, las mismas variables de entorno y los mismos ficheros abiertos. Eso es todo lo que hay. Usualmente, a continuación el proceso hijo ejecuta execve o una llamada al sistema similar para cambiar su imagen de memoria y pasar a ejecutar un nuevo programa. Por ejemplo cuando un usuario teclea un comando del shell como por ejemplo, sort, el shell ejecuta un fork para crear un proceso hijo, el cual es el que realmente ejecuta el programa correspondiente al sort. La razón de realizar estos dos pasos es permitir al hijo que manipule los descriptores de fichero del shell después del fork pero antes de que el execve lleve a cabo la redirección de la entrada estándar, la salida estándar y la salida de errores estándar.

Lo anterior contrasta con **lo que sucede en Windows**, donde mediante una única llamada al sistema de Win32, CreateProcess, se realiza tanto la creación del proceso como la carga del programa correcto dentro del nuevo proceso. Esta llamada tiene 10 parámetros que incluyen entre ellos el programa que hay que ejecutar, los parámetros de la línea de comandos que va a recibir el programa, varios atributos de seguridad, bits que controlan si se heredan los ficheros abiertos, información sobre la prioridad del proceso, una especificación de la ventana que hay que crear (en su caso) para el proceso, y un puntero a una estructura (un registro) en la que se envíe de retorno toda la información sobre el nuevo proceso creado, al proceso que hace la llamada. Adicionalmente a CreateProcess, Win32 cuenta con unas 100 llamadas al sistema más, para gestionar y sincronizar los procesos, así como para operaciones relacionadas



Tanto en UNIX como en Windows, después de crear un proceso, tanto el padre como el hijo cuentan con sus propios espacios de direcciones disjuntos. Si cualquiera de los procesos modifica una palabra en su espacio de direcciones, ese cambio es invisible para cualquier otro proceso. **En UNIX, el espacio de direcciones inicial del hijo es una copia del espacio de direcciones del padre, pero hay dos espacios de direcciones distintos involucrados; la memoria no escribible se comparte** (algunas implementaciones de UNIX comparten el área de código entre los dos, ya que el código nunca se modifica). Sin embargo es posible que un nuevo proceso creado comparta algunos de los demás recursos del padre, tales como los ficheros abiertos. **En Windows, los espacios de direccionamiento del padre y el hijo son diferentes desde el primer momento.**


== Terminacion de los procesos ==
Tras la creacion de un proceso comienza su ejecución realizando el trabajo que se le ha encomendado. Sin embargo nada dura para siempre, ni siquiera los procesos. Pronto o tarde el nuevo proceso debe terminar, usualmente debido a una de las siguientes causas.
1- El proceso completa su trabajo y termina (voluntariamente).
2- El proceso detecta un error y termina (voluntariamente).
3- El sistema detecta un error fatal del proceso y fuerza su terminacion.
4- Otro proceso fuerza la terminacion del proceso (por ejemplo en UNIZ mediante la llamada al sistema KILL)

La mayoría de los procesos terminan debido a que han completado su trabajo. Cuando un compilador ha compilado el programa que se le ha dado, el compilador ejecuta una llamada al sistema para decirle al sistema operativo que ha finalizado. Esta llamada es exit en UNIX y ExitProcess en Windows. Los programas orientados a la pantalla soportan también la terminación voluntaria. Los procesadores de texto, navegadores y programas similares cuentan siempre con un icono o una opción de menú para que el usuario pueda pinchar con el ratón indicándole al proceso que borre cualquier fichero temporal que esté abierto y a continuación
termine.

Razones por las que termina un proceso:
- Errores de ejecucion, por ejemplo que no exista el fichero que se quiere ejecutar
- Errores de programacion en el programa
- Es posible que un proceso haga una llamada a sistema para terminar otro proceso

=== Jerarquías de Procesos ===
En algunos sistemas, cuando un proceso crea otro proceso, el proceso padre y el proceso hijo, continúan estando asociados de cierta manera. El proceso hijo puede a su vez crear más procesos formando una jerarquía de procesos. De forma diferente a las plantas y animales que se reproducen de forma sexual, un proceso tiene un único padre (pero cero, uno, dos o más hijos). En UNIX, un proceso y todos sus hijos y demás descendientes forman juntos un grupo de procesos. 
Cuando un usuario envía una señal desde el teclado (como por ejemplo tecleando Ctrl-C), la señal se propaga a todos los miembros del grupo de procesos actualmente asociados con el teclado (normalmente todos los procesos activos que fueron creados en la ventana actual). Individualmente, cada proceso puede capturar la señal, ignorar la señal o emprender la acción por defecto, que es la de ser matado por la señal recibida.
Otro ejemplo del papel que puede jugar la jerarquía de procesos es cómo se inicializa UNIX durante su arranque. Hay un proceso especial presente en la imagen de arranque denominado init. Cuando este proceso comienza su ejecución lee un fichero donde figura el número de terminales con que cuenta el sistema. Acto seguido init crea mediante la llamada al sistema fork un nuevo proceso por terminal. Estos procesos esperan a que alguien se conecte al sistema a través del correspondiente terminal. Cada vez que un usuario logra conectarse, el proceso asociado al terminal ejecuta un shell para aceptar comandos. A su vez estos comandos pueden dar lugar a la creación de más procesos. En definitiva, todos los procesos en el sistema pertenecen a un único árbol que tiene al proceso init como raíz. 
Por el contrario, Windows no ofrece ningún concepto de jerarquía de procesos. Todos los procesos son iguales. El único lugar donde hay algo parecido a una jerarquía de procesos es
que cuando se crea un proceso, su proceso padre recibe un puntero a un conjunto de información (lo que se denomina un handle) que puede utilizar para controlar al proceso hijo. Sin embargo, el padre es libre de pasar o no esa información a algún otro proceso, lo que significa que no está asegurado por el sistema el mantenimiento de la jerarquía de los procesos creados. 
Los procesos en UNIX no pueden desentenderse de sus hijos

=== Estado de los procesos ===
Un proceso puede tener 3 estados , bloqueado preparado y en ejecucion. 
Bloqueado: no puede ejecutarse porque le faltan datos (quizas output de otro proceso que tomara como input) 
Preparado: listo para ser ejecutado, a la espera de que le asigenen procesador
En ejecucion: utilizando procesador en ese momento

== Pueden darse 4 transiciones ==
1- De ejecucion a bloqueado
2- De bloqueado a preparado
3- De preparado a ejecucion
4- Seleccion del proceso preparado que se va a ejecutar



=== Implementacion de procesos ===
Para implementar el modelo de los procesos , el sistema operativo mantiene una tabla (un array de registros o estructuras), denominada tabla de procesos, con una entrada por proceso.
Algunos autores denominan a cada una de esas entradas descriptor de proceso o bloque de control de proceso.
Estas entradas contienen informacion sobre el estado de cada proceso, su contador de programa, su puntero de pila, su asignacion de memoria, el estado de sus ficheros abiertos, la informacion relativa a su planificacion y a la contabilidad de recursos que ha consumido, asi como cualquier otra informacion sobre el proceso que deba guardarse cuando el proceso conmute del estado en ejecucion al estado de preparado o bloqueado, de forma que su ejecucion pueda retomarse posteriormente como si nunca se hubiera detenido.


Cada clase de dispositivos de E/S (como por ejemplo las disqueteras, los discos duros, los timers o los terminales) tiene asociada una posición de memoria (a menudo situada en las posiciones más bajas de la memoria) que se denomina el vector de interrupción utilizado por ese tipo de dispositivos. Esta posición de memoria contiene la dirección de la rutina de tratamiento de la interrupción que atiende a ese tipo de dispositivos. Supongamos que el proceso de usuario número 3 se está ejecutando cuando de repente la CPU recibe una interrupción del disco duro. En ese momento el hardware de las interrupciones apila (en la pila actual) el contador de programa del proceso de usuario número 3, la palabra de estado del programa (es decir su registro de estado) y posiblemente algunos otros registros hardware mas. A continuación la CPU salta a la dirección especificada en el vector de interrupción del disco. Eso es todo lo que hace el hardware. Desde ese momento toma el control el software, más concretamente la rutina de tratamiento de la interrupción.

Todas las interrupciones comienzan salvando los registros (a menudo en el descriptor del proceso actualmente en ejecución). A continuación se desapila la información apilada anteriormente por el hardware (en el momento de la interrupción), estableciéndose el puntero de pila para que apunte a una pila temporal utilizada por el proceso controlador (en este caso del disco). Las acciones anteriores, tales como salvar los registros y establecer el puntero de pila, no pueden expresarse en lenguajes de alto nivel como C, de manera que las realiza una pequeña rutina en lenguaje ensamblador, usualmente la misma para todas las interrupciones ya que el trabajo de salvar los registros es siempre el mismo, sin importar cuál sea la causa de la interrupción.
Cuando esta rutina termina, realiza una llamada a un procedimiento en C para llevar a cabo el resto del trabajo para este tipo de interrupción específico. Estamos suponiendo que el sistema operativo está escrito en C (que es la elección usual para todos los sistemas operativos reales). Cuando ese procedimiento termina su trabajo, posiblemente provocando que algún proceso pase al estado de preparado, se llama al planificador para determinar qué proceso preparado se ejecuta a continuación. Después de eso, se devuelve el control al código en lenguaje ensamblador para cargar los registros y el mapa de memoria del nuevo proceso que ha seleccionado el planificador y se retoma su ejecución. En la Figura 2-5 se resume el tratamiento de las interrupciones y la planificación. Es necesario insistir en que los detalles varían considerablemente de un sistema operativo a otro.

**1. El hardware apila el contador de programa, etc.**
**2. El hardware carga el nuevo contador de programa desde el vector de interrupción.**
**3. Una rutina de lenguaje ensamblador salva los registros.**
**4. Una rutina de lenguaje ensamblador establece una nueva pila.**
**5. Se ejecuta la rutina de tratamiento de la interrupción escrita en C (normalmente lee y guarda en un búfer el dato de entrada).**
**6. El planificador decide qué procedimiento ejecutar a continuación.**
**7. Un procedimiento escrito en C retorna al código en ensamblador.**
**8. Una rutina de lenguaje ensamblador (el dispatcher) pasa a ejecución el proceso seleccionado por el planificacor.**


== Threads - Hilos ==
En los sistemas operativos tradicionales, cada proceso tiene su propio espacio de direcciones y un único flujo (hilo) de control. De hecho, casi es esa la definición de proceso. Sin embargo, frecuentemente hay situaciones en las que es deseable contar con múltiples hilos de control (threads) en el mismo espacio de direcciones ejecutándose quasi-paralelamente, como si fueran procesos separados (excepto que comparten el mismo espacio de direcciones).


=== El Modelo de los Threads ===
Como hemos expuesto largamente, el modelo de los procesos se basa en dos conceptos independientes: el agrupamiento de los recursos y la ejecución secuencial de un programa. A veces es útil separar esos dos conceptos, y es aquí donde entran en juego los threads.
Una forma de ver un proceso es que es una manera de agrupar juntos recursos relacionados. Un proceso tiene un espacio de direcciones conteniendo código y datos del programa, así como otros recursos. Estos recursos pueden incluir ficheros abiertos, procesos hijos, alarmas pendientes, controladores de señales, información de contabilidad, etc. Poniendo juntos todos esos recursos en la forma de un proceso, es posible gestionarlos más fácilmente. El otro concepto que incluye un proceso es el de hilo de ejecución, usualmente denominado un thread. El thread tiene un contador de programa que indica cuál es la siguiente instrucción a ejecutar. Tiene además registros que contienen sus variables de trabajo actuales.
Tiene una pila, que contiene una especie de historia de su ejecución, con una trama por cada procedimiento al que se ha llamado pero del que no se ha retornado todavía. Aunque un thread debe ejecutarse en algún proceso, el thread y su proceso son conceptos diferentes y pueden tratarse de forma separada. Los procesos se utilizan para agrupar recursos juntos; los threads son las entidades planificadas para su ejecución en la CPU.

Lo que los threads añaden al modelo de los procesos es que permiten que hayamúltiples ejecuciones en un mismo entorno determinado por un proceso, y esto con un alto grado de independencia de esas ejecuciones. Tener múltiples threads ejecutándose en paralelo dentro de un proceso es análogo a tener múltiples procesos ejecutándose en paralelo dentro de un ordenador. En el primer caso, los threads comparten el espacio de direcciones, los ficheros abiertos y otros recursos. En el segundo caso, los procesos comparten la memoria física, los discos, las impresoras y otros recursos. Debido a que los threads tienen algunas de las propiedades de los procesos, a veces reciben la denominación de procesos ligeros (lightweight process). También se utiliza el término de multihilo (multithreaded) para describir la situación en la cual se permite que haya múltiples threads en el mismo proceso.


Cuando un proceso multihilo se ejecuta sobre un sistema con una única CPU, los threads deber hacer turnos para ejecutarse. En la Figura 2-1 vimos como funciona la multiprogramación de procesos. El sistema operativo crea la ilusión de que hay varios procesos secuenciales que se ejecutan en paralelo a base de ir conmutando la CPU entre esos procesos. Los sistemas multihilo funcionan de la misma manera. La CPU va conmutándose rápidamente de unos threads a otros proporcionando la ilusión de que los threads se están ejecutando en paralelo, aunque sobre una CPU virtual más lenta que la real. Con tres threads intensivos en computación dentro de un proceso, los threads aparentan estar ejecutándose en paralelo, cada uno sobre una CPU con un tercio de la velocidad de la CPU real.

Los diferentes threads de un proceso no son tan independientes como si fueran diferentes procesos. Todos los threads tienen exactamente el mismo espacio de direcciones, lo que significa en particular que comparten las mismas variables globales. Ya que cualquier thread puede acceder a cualquier dirección de memoria dentro del espacio de direcciones del proceso, un thread puede leer, escribir o incluso borrar completamente la pila de cualquier otro thread. No existe ninguna protección entre los threads debido a que (1) es imposible establecer ninguna medida protección, y (2) es innecesario que haya protección. De forma distinta que con diferentes procesos, que pueden corresponder a diferentes usuarios y que pueden ser hostiles uno con otro, un proceso sólo puede tener un único usuario propietario, quien cuando crea múltiples threads lo hace presumiblemente con la idea de que puedan cooperar, no luchar.

Los elementos de la primera columna de la Figura 2-7 corresponden a propiedades de los procesos y no a propiedades de los threads. Por ejemplo si un thread abre un fichero, ese fichero es visible por todos los otros threads del proceso, que pueden ponerse inmediatamente a leer y escribir en él. Esto es lógico ya que el proceso es la unidad de gestión de recursos, y no el thread. Si cada thread tuviera su propio espacio de direcciones, ficheros abiertos, alarmas pendientes, etc. se trataría de un proceso separado. Lo que estamos tratando de conseguir con el concepto de thread es la capacidad de que múltiples hilos de ejecución compartan un conjunto de recursos de manera que puedan trabajar estrechamente juntos para realizar alguna tarea.

Igual que un proceso tradicional (es decir un proceso con un único thread), un thread puede estar en cualquiera de los estados: en ejecución, bloqueado, preparado o terminado. Un thread en ejecución tiene actualmente la CPU y está activo. Un thread bloqueado está esperando a que algún suceso lo desbloquee. Por ejemplo cuando un thread realiza una llamada al sistema para leer del teclado, ese thread se queda bloqueado hasta que se presiona alguna tecla. Un thread puede bloquearse esperando a que tenga lugar algún suceso externo o a que algún otro thread lo desbloquee. Un thread preparado está planificado para ejecutarse y lo hace tan pronto como le llega su turno. Las transiciones entre los estados de un thread son las mismas que las transiciones entre los estados de un proceso.

Sin embargo, si el proceso hijo se crea con el mismo número de threads que el padre, ¿qué sucede si un thread del padre se bloquea en una llamada a read, por ejemplo del teclado? ¿Resulta ahora que los dos threads están ahora bloqueados por el teclado, uno en el padre y otro en el hijo? Cuándo se termina de teclear una línea, ¿obtienen los dos threads una copia de esa línea? ¿Sólo el padre? ¿Sólo el hijo? Este mismo problema se plantea también en relación con las conexiones de red abiertas.

Otra clase de problemas se relaciona con el hecho de que los threads comparten numerosas estructuras de datos. ¿Qué sucede si un thread cierra un fichero mientras otro thread está leyendo todavía? Supongamos que un thread se da cuenta de que queda demasiado poca memoria disponible y empieza a hacer acopio de más memoria. Si en ese momento tiene lugar un cambio de thread en ejecución y el nuevo thread aprecia también que queda demasiado poca memoria, comenzará a su vez a reservar más memoria. En esa situación es muy probable que se asigne el doble de la memoria necesaria. Estos problemas pueden resolverse con algún esfuerzo, pero requieren un análisis y diseño cuidadoso para conseguir que los programas multihilo funcionen correctamente.


=== Utilizacion de los Threads ===
Habiendo descrito qué son los threads, es el momento de exponer la razón de porqué es deseable disponer de ellos en el sistema operativo. La razón principal para tener threads es que son numerosas las aplicaciones en las que hay varias actividades que están en marcha simultánemente. De vez en cuando, alguna de estas actividades puede bloquearse. 

En esa situación el modelo de programación resulta más sencillo si descomponemos tal aplicación en
varios threads secuenciales que se ejecutan en paralelo. Hemos visto este argumento antes. Es precisamente el mismo argumento que hicimos para justificar la conveniencia de disponer de procesos. 
En vez de pensar en términos de interrupciones, timers y cambios de contexto, podemos pensar en términos de procesos paralelos. Sólo que ahora con los threads introducimos un nuevo elemento: la capacidad de las entidades paralelas para compartir entre ellas un espacio de direcciones y todos sus datos.
 Esta capacidad es esencial para ciertas aplicaciones, y es por lo que el tener simplemente múltiples procesos (con sus espacios de direcciones separados) no puede funcionar en estos casos. Un segundo argumento para tener threads es que ya que no tienen ningún recurso ligado a ellos, son más fáciles de crear y destruir que los procesos. En numerosos sistemas, la creación de un thread puede realizarse 100 veces más rápido que la creación de un proceso. Cuando el número de threads necesita cambiar dinámica y rápidamente, esa propiedad es efectivamente útil.
Una tercera razón para tener threads es también un argumento sobre el rendimiento. Los threads no proporcionan ninguna ganancia en el rendimiento cuando todos ellos utilizan intensamente la CPU, sino cuando hay una necesidad substancial tanto de cálculo en la CPU como de E/S, de manera que teniendo threads se puede conseguir que esas dos actividades se solapen, acelerando la ejecución de la aplicación. Finalmente, los threads son útiles sobre sistemas con varias CPUs, donde es posible un paralelismo auténtico.


=== Ejemplo de servidor de un solo hilo ===
Hasta aquí hemos visto dos diseños posibles: un servidor web multihilo y un servidor web con un único thread. Supongamos que los threads no están disponibles pero que los diseñadores del sistema se encuentran con que es inaceptable la pérdida de eficiencia debida a la utilización de un único thread. Si el sistema dispone de una versión no bloqueante de la llamada al sistema read, es posible un tercer enfoque del problema. Cuando entra una nueva petición el único thread existente la examina. Si puede satisfacerse desde la caché, perfecto, pero si no es así, se arranca una operación no bloqueante de lectura del disco. .

El servidor guarda el estado de la petición actual en una tabla y pasa a atender el siguiente evento. El siguiente evento puede ser bien una nueva petición de otra página web, o una respuesta del disco a una operación de lectura previa. Si se trata de una nueva petición, se atiende como la anterior. Si se trata de una respuesta del disco, se extrae de la tabla la información relevante y se procesa la respuesta. Con E/S del disco no bloqueante, las respuestas del disco tendrán probablemente la forma de una señal o una interrupción.

En este diseño, el modelo de los “procesos secuenciales” que teníamos en los dos primeros casos se pierde. El estado del procesamiento de las peticiones debe salvarse y restaurarse explícitamente cada vez que el servidor conmuta de trabajar sobre una  petición a trabajar sobre otra. 
Realmente lo que se está haciendo es simular los threads y sus pilas de la manera más difícil. Un diseño como este en el que cada procesamiento tiene un estado guardado y existe algún conjunto de eventos que pueden suceder para alterar su estado se denomina una máquina de estados finitos (o un autómata finito determinista). Este concepto se utiliza muy a menudo en informática.


Los threads ofrecen una solución. El proceso puede estructurarse en un thread de entrada, un thread de procesamiento y un thread de salida. El thread de entrada lee los datos dejándolos en un búfer de entrada. El thread de procesamiento toma datos del búfer de entrada, los procesa, y deja el resultado en un búfer de salida. El thread de salida escribe esos resultados en el disco. De esta manera la entrada, la salida y el procesamiento pueden ir haciéndose al mismo tiempo. Por supuesto, este modelo sólo funciona si una llamada al sistema bloquea sólo al thread que la invoca, y no al proceso entero que contiene a ese thread.




=== Implementacion de los Threads en el espacio del usuario ===
Existen dos formas fundamentales de implementar un paquete de threads: en el espacio del usuario y en el núcleo (kernel). La elección entre esas dos implementaciones resulta moderadamente controvertida, siendo también posible una implementación híbrida. Vamos a pasar a describir estos métodos, junto con sus ventajas y desventajas. 
El primer método consiste en poner el paquete de threads enteramente en el espacio de usuario. En consecuencia el núcleo del sistema no sabe nada de su existencia. En lo que concierne al núcleo, sólo se gestionan procesos ordinarios con un único thread. La primera, y más obvia, ventaja es que el paquete de threads a nivel de usuario puede implementarse sobre un sistema operativo que no soporte threads. Todos los sistemas operativos tradicionales entran dentro de esta categoría, y algunos de ellos siguen incluso actualmente sin dar soporte a los threads.



{{.\pasted_image001.png}}{{.\pasted_image002.png}}
  




Cuando los threads se gestionan en el espacio del usuario, cada proceso necesita su propia tabla de threads privada para llevar el control de sus threads. Esta tabla es análoga a la tabla de procesos del núcleo, salvo que sólo controla las propiedades propias de los threads tales como el contador de programa del thread, su puntero de pila, sus registros, su estado, etc. La tabla de threads está gestionada por el sistema en tiempo de ejecución. Cuando un thread pasa al estado preparado o bloqueado, la información necesaria para proseguir posteriormente con su ejecución se guarda en la tabla de threads, exactamente de la misma forma que el núcleo almacena la información sobre los procesos en la tabla de procesos.

Cuando un thread hace algo que puede provocar que se bloquee localmente, como por por ejemplo, esperar a que otro thread en su mismo proceso complete algún trabajo, entonces llama a un procedimiento del sistema en tiempo de ejecución. Este procedimiento comprueba si el thread debe pasar al estado bloqueado. Si es así, el procedimiento guarda los registros del thread (que son los mismos que los suyos propios) en la tabla de threads, busca en la tabla un thread preparado para ejecutarse, y recarga los registros de la máquina con los valores que se tienen guardados correspondientes al nuevo thread. Tan pronto como se conmutan el puntero de pila y el contador de programa, el nuevo thread vuelve automáticamente otra vez a la vida. Si la máquina tiene una instrucción para guardar todos los registros, y otra para cargarlos todos, la conmutación de un thread a otro puede hacerse ejecutando un puñado de instrucciones. Este tipo de conmutación de threads es al menos un orden de magnitud más rápido que hacer un trap al núcleo, lo que representa un potente argumento a favor de los paquetes de threads a nivel de usuario.

Sin embargo existe una diferencia clave con respecto a los procesos. Cuando un thread termina de ejecutarse por el momento, por ejemplo cuando realiza una llamada a thread_yield, el código de thread_yield puede salvar él mismo la información del thread en la tabla de threads. Además, puede llamar al thread planificador para que escoja otro thread para pasarlo a ejecución. El procedimiento que salva el estado del thread y el planificador no son más que procedimientos locales, por lo que su invocación es mucho más eficiente que realizar una llamada al núcleo. Entre otras cuestiones, no es necesario ningún trap, no es necesario ningún cambio de contexto, no es necesario vaciar la memoria caché, etc. Esto hace que la planificación de los threads sea muy rápida.

Los threads a nivel de usuario tienen también otras ventajas. Para empezar, permiten que cada proceso tenga su propio algoritmo de planificación ajustado a sus necesidades. Para
algunas aplicaciones, como por ejemplo aquellas con un thread recolector de basura, es una ventaja adicional el no tener que preocuparse sobre si un thread se ha quedado detenido en un momento inadecuado. También se dimensionan mejor, ya que los threads a nivel del núcleo requieren invariablemente en el núcleo algún espacio para la tabla de threads y algún espacio de pila, lo que puede resultar un problema cuando el número de threads es muy grande.
A pesar de su mayor eficiencia, los paquetes de threads a nivel de usuario tienen algunos serios problemas. El primero de ellos es el problema de cómo se implementan las llamadas al sistema bloqueantes. Supongamos que un thread lee desde el teclado antes de que se haya pulsado ninguna tecla. Es inaceptable permitir que el thread haga realmente esa llamada al sistema, ya que eso detendría a todos los threads del proceso. Uno de los principales motivos con que hemos justificado la necesidad de incorporar al sistema los threads fue el permitir utilizar llamadas bloqueantes pero sin que el bloqueo de un thread afectase a los demás. Con llamadas al sistema bloqueantes, es difícil imaginarse cómo puede conseguirse este objetivo fácilmente.

Es posible otra alternativa en aquellos casos en los que es posible determinar con antelación si una llamada al sistema va a producir un bloqueo. En algunas versiones de UNIX, existe una llamada al sistema, select, que permite al que la invoca saber si una determinada llamada read que se propone realizar va a bloquearse o no. Cuando se dispone de esta llamada, el procedimiento de librería read puede reemplazarse por uno nuevo que primero realiza una llamada a select y luego hace la llamada a read si es segura (es decir si no va a producir un bloqueo). Si por el contrario la llamada read va a producir un bloqueo, la llamada no se hace, y en su lugar se ejecuta otro thread. La siguiente vez que el sistema en tiempo de ejecución tome el control, puede comprobar de nuevo si la llamada read es ya segura. Este enfoque requiere reescribir partes de la librería de llamadas al sistema, es ineficiente y nada elegante, pero no queda otra elección. El código añadido alrededor de la llamada al sistema para hacer las comprobaciones sobre la seguridad de la llamada se denomina un jacket (chaqueta) o wrapper (envoltorio).

Un problema análogo al de las llamadas al sistema bloqueantes es el problema de las faltas de página. Estudiaremos ese problema en el capítulo 4, pero de momento es suficiente con decir que los ordenadores pueden configurarse de forma que no todo el programa esté en memoria principal a la vez. Si el programa llama o salta a una instrucción que no está en memoria, tiene lugar una falta de página y el sistema operativo debe ir y tomar las instrucciones no encontradas (y sus vecinas) obteniéndolas del disco. Eso es lo que se denomina una falta de página. 
El proceso se bloquea mientras se localizan y leen las instrucciones necesarias. Si un thread provoca una falta de página, el núcleo, que ni siquiera sabe de la existencia de los threads, bloquea en consecuencia al proceso entero hasta que la E/S del disco provocada por la  alta de página se complete, y eso incluso aunque haya otros threads ejecutables dentro del proceso.

Otro problema con los paquetes de threads a nivel de usuario es que si un thread comienza a ejecutarse, ningún otro thread en ese proceso podrá volver a ejecutarse mientras que el primer thread no ceda voluntariamente la CPU. Dentro de un único proceso, no existen interrupciones de reloj, lo que hace imposible planificar los threads de una forma tipo round-robin (es decir turnándose periódicamente). A menos que un thread entre en el sistema en tiempo de ejecución por su propia voluntad, el planificador nunca tiene la oportunidad de pasar a ejecución a otro thread.


Una posible solución al problema de la ejecución indefinidamente prolongada de los threads es hacer que el sistema en tiempo de ejecución solicite una señal de reloj (interrupción) una vez cada segundo para obtener el control, pero eso es también algo rudimentario y complicado de programar. Las interrupciones periódicas del reloj no siempre son posibles con una alta frecuencia, pero incluso aunque lo sean, generan una considerable sobrecarga total en el sistema. Además, un thread también puede necesitar su propia interrupción de reloj, la cual podría interferir con el uso que el sistema en tiempo de ejecución estaría ya haciendo del reloj.

Otro, y probablemente el argumento más demoledor en contra de a los threads a nivel de usuario es que, en general, los programadores desean utilizar los threads precisamente en las aplicaciones donde los threads se bloquean muy a menudo, como por ejemplo en un servidor web multihilo. Esos threads están constantemente haciendo llamadas al sistema. Una vez que el thread ha hecho un trap al núcleo para llevar a cabo una llamada al sistema, no significa mucho más trabajo para el núcleo conmutar a otro thread si el primero se bloquea, y dejar que el núcleo haga eso elimina la necesidad de hacer constantemente llamadas al sistema select para comprobar si las llamadas al sistema read son seguras. Para aplicaciones que esencialmente mantienen muy ocupada la CPU bloqueándose raramente, ¿cuál es la ventaja de disponer de threads? Nadie puede proponer seriamente calcular los n primeros números primos o jugar al  ajedrez utilizando threads, debido a que no hay nada que ganar haciéndolo de esa manera.


=== Implementacion de Threads en el nucleo ===
Ahora vamos a considerar el caso en el que el núcleo conoce y gestiona los threads. En ese caso, como se muestra en la Figura 2-13(b), no es necesario ningún sistema en tiempo de ejecución dentro de cada proceso. Igualmente no existe ninguna tabla de threads en cada proceso. En vez de eso, el núcleo mantiene una tabla de threads que sigue la pista de todos los threads en el sistema. Cuando un thread desea crear un nuevo thread o destruir uno que ya existe, hace una llamada al núcleo, que es el que se encarga efectivamente de su creación o destrucción actualizando la tabla de threads del sistema.

La tabla de threads del núcleo guarda los registros de cada thread, su estado y otra información. La información es la misma que con threads a nivel de usuario, pero ahora esa información está en el núcleo en vez de en el espacio de usuario (dentro del sistema en tiempo de ejecución). Esta información es un subconjunto de la información que los núcleos tradicionales mantienen sobre cada uno de sus procesos con un solo thread, esto es, el estado del proceso. Adicionalmente, el núcleo mantiene también la tabla de procesos para seguir la pista de los procesos.

Todas las llamadas que puedan bloquear a un thread se implementan como llamadas al sistema, a un coste considerablemente más alto que una llamada a un procedimiento del sistema en tiempo de ejecución. Cuando se bloquea un thread, el núcleo tiene la opción de decidir si pasa a ejecutar otro thread del mismo proceso (si es que hay alguno preparado), o pasa a ejecutar un thread de un proceso diferente. Con threads a nivel de usuario el sistema en tiempo de ejecución tiene que seguir ejecutando threads de su propio proceso hasta que el núcleo le arrebate la CPU (o hasta que no le queden threads preparados para ejecutarse).


Debido al coste relativamente alto de crear y destruir los threads en el núcleo, algunos sistemas toman un enfoque medio-ambientalmente correcto reciclando sus threads. Cuando se destruye un thread, éste se marca como un thread no ejecutable, pero sin que se vean afectadas de otro modo sus estructuras de datos del núcleo. Posteriormente, cuando haya que crear un nuevo thread, se procederá a reactivar un antiguo thread marcado, ahorrando de esta manera algo de la sobrecarga de la creación normal de un thread. También es posible el reciclado de threads para threads a nivel de usuario, pero ya que la sobrecarga de la gestión de los threads es
mucho menor, el incentivo para reciclar resulta menos atractivo. Los threads a nivel del núcleo no requieren ninguna llamada al sistema nueva de tipo no bloqueante. Además, si un thread de un proceso provoca una falta de página, el núcleo puede comprobar fácilmente si el proceso tiene todavía threads ejecutables, y en ese caso pasar a ejecutar uno de ellos mientras espera a que la página que provocó la falta se cargue desde el disco. Su principal desventaja es que el coste de una llamada al sistema es considerable, de manera que si las operaciones con threads (creación, terminación, etc.) son frecuentes, puede incurrirse en una elevada sobrecarga para el sistema.


=== Implementaciones hibridas ===
Se han investigado varias líneas que intentan combinar las ventajas de los threads a nivel de usuario con las ventajas de los threads a nivel del núcleo. Una línea consiste en utilizar threads a nivel del núcleo y multiplexar threads a nivel de usuario sobre algunos o todos los threads a nivel del núcleo.

{{.\pasted_image003.png}}




En este diseño el núcleo sólo tiene tiene conocimiento de los threads a nivel del núcleo, ocupándose de su planificación. Algunos de estos threads pueden tener multiplexados sobre ellos múltiples threads a nivel de usuario. Estos threads a nivel de usuario se crean, se destruyen y se planifican de la misma manera que los threads a nivel de usuario de un proceso que se ejecuta sobre un sistema operativo sin capacidad multihilo. En este modelo, cada thread a nivel de núcleo tiene algún conjunto de threads a nivel de usuario que lo utilizan por turnos para ejecutarse.


=== Activaciones del Planificador ===
Los objetivos del trabajo sobre activaciones del planificador son imitar la funcionalidad de los threads a nivel del núcleo, pero con el mejor rendimiento y la mayor flexibilidad usualmente asociada con los paquetes de threads implementados en el espacio del usuario. En particular, los threads a nivel del usuario no tienen que hacer llamadas al sistema especiales no bloqueantes o comprobar por adelantado si es seguro realizar ciertas llamadas al sistema. Sin embargo, cuando un thread se bloquea en una llamada al sistema o debido a una falta de página, debe ser posible ejecutar otros threads dentro del mismo proceso, supuesto que haya algún thread preparado.





